<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Wenzhe Zhai</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Wenzhe Zhai">
    <meta name="author" content="Wenzhe Zhai">

    <meta name="keywords" content="Wenzhe Zhai, W. Zhai" />

    <!-- Le styles -->
    <link href="./css/bootstrap.css" rel="stylesheet">
    <link href="./css/bootstrap-responsive.css" rel="stylesheet">
    <link href="./prettify.css" rel="stylesheet">
    <link href="./css/docs.css" rel="stylesheet">
    <link href="./css/aux.css" rel="stylesheet">

    <!-- Fav and touch icons -->
    <link rel="shortcut icon" href="./images/icon/fav.png">
    <link rel="apple-touch-icon-precomposed" sizes="144x144"
        href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114"
        href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72"
        href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed"
        href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-57-precomposed.png">

</head>

<body>

    <!-- Navigation ================================================== -->
    <div class="navbar navbar-inverse navbar-fixed-top content_width_range">
        <div class="navbar-inner">
            <div class="container">
                <button type="button" class="btn btn-navbar collapsed" data-toggle="collapse"
                    data-target=".nav-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="brand" href="#">Wenzhe Zhai</a>
                <div class="nav-collapse collapse">
                    <ul class="nav">
                        <li class="active"><a href="#"><strong>Home</strong></a></li>
                        <!-- <li><a href="./publications.html"><strong>Publications</strong></a></li>
                        <li><a href="./services.html"><strong>Services</strong></a></li> -->
                        <!-- <li><a href="./news.html"><strong>News</strong></a></li> -->
                    </ul>
                </div>
            </div><!--/.container-->
        </div>
    </div>


    <!-- Profile Image & About Me ================================================== -->
    <div class="container">
        <div class="container-fluid content_width_range">
            <div class="row-fluid basic_info">
                <!-- Profile Image and Text -->
                <div class="span2">
                    <ul class="">
                        <div style="text-align: center;">
                            <img style="width: 100%;" src="./images/WenzheZhai.jpg" alt="Wenzhe Zhai" />
                            <h3>Wenzhe Zhai</h3>
                        </div>
                    </ul>

                </div>

                <!-- About Me -->
                <div class="span6">
                    <div class="aboutme">

                        <p>
                            Wenzhe Zhai is a first year Ph.D. student in College of Intelligent Systems Science and Engineering, <a
                            href="http://www.hrbeu.edu.cn/">Harbin Engineering University</a>, supervised by Prof. 
                            <a
                            href="https://homepage.hrbeu.edu.cn/web/xingxianglei/">Xianglei Xing</a>.
                            Previously, he received the M.S. degree form <a href="https://www.sdut.edu.cn">Shandong
                            University of Technology</a> in 2023, supervised by Prof. 
                            <a
                            href="https://gao.brighten.group/">Mingliang Gao</a>.
                            His research interests include computer vision and deep learning, particularly focusing on
                            object counting.
                        </p>
                        <strong>Email:</strong> wenzhezhai AT outlook.com
                    </div>

                </div>


                <div class="span1">
                    <br />
                    <a class="label_download"
                        href="https://scholar.google.com/citations?user=DDnlrU8AAAAJ&hl=zh-CN">Google
                        Scholar</a>
                    <!-- <a class="label_download" href="https://www.researchgate.net/profile/Qilei-Li">ResearchGate</a> -->
                    <br>
                    <a class="label_download" href="https://orcid.org/0000-0003-0996-6832">ORCID</a>
                    <a class="label_download" href="https://github.com/wenzhezhai">GitHub</a>
                    <br>
                </div>

            </div>

        </div>


        <!-- ============================ Recent Publications ======================== -->

        <div class="page-header">
            <!-- <h3>Publications [ <a class="" href="publications.html">Full List</a> ] </h3> -->
            <h3>Publications </h3>
        </div>

        <div id="latest_publications" class="container-fluid content_width_range">
            <ol>
                <li>
                    Zero-shot Object Counting with Vision-Language Prior Guidance Network<br>
                    <strong>Wenzhe Zhai</strong>, Xianglei Xing, Mingliang Gao, Qilei Li<br>
                    <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, October 2024,  <a style="color: black;"><strong>IF=8.3</strong></a>, <strong>CCF-B</strong></a>
                    <br>
                    [ <a href='papers/ZhaiEtAl_TCSVT2024_Count.pdf'>PDF</a> ]
                    [ <a href='https://doi.org/10.1109/TCSVT.2024.3488721'>Link</a> ]
                </li>
                
                <li>
                    Multi-view gait recognition with joint local multi-scale and global contextual spatio-temporal features<br>
                    <strong>Wenzhe Zhai</strong>, Haomiao Li, Chaoqun Zheng, Xianglei Xing<br>
                    <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, October 2024,  <a style="color: black;"><strong>IF=8.3</strong></a>, <strong>CCF-B</strong></a>
                    <br>
                    [ <a href='papers/ZhaiEtAl_TCSVT2024_Gait.pdf'>PDF</a> ]
                    [ <a href='https://ieeexplore.ieee.org/document/10707343'>Link</a> ]
                </li>
                
                <li>
                    Scale-Context Perceptive Network for Crowd Counting and Localization in Smart
                    City System<br>
                    <strong>Wenzhe Zhai</strong>, Mingliang Gao, Xiangyu Guo, Qilei Li<br>
                    <em>IEEE Internet of Things Journal</em>, April 2023,  <a style="color: black;"><strong>IF=10.6</strong></a>
                    <br>
                    [ <a href='papers/ZhaiEtAl_IITJ2023.pdf'>PDF</a> ]
                    [ <a href='https://ieeexplore.ieee.org/abstract/document/10105438'>Link</a> ]
                </li>
               
                <li>
                    Region-Aware Quantum Network for Crowd Counting<br>
                    <strong>Wenzhe Zhai</strong>, Xianglei Xing, Gwanggil Jeon<br>
                    <em>IEEE Transactions on Consumer Electronics</em>, March 2024
                    <br>
                    [ <a href='papers/ZhaiEtAl_TCE2023.pdf'>PDF</a> ]
                    [ <a href='https://ieeexplore.ieee.org/abstract/document/10478718'>Link</a> ]
                </li>

                <li>
                    FPANet: feature pyramid attention network for crowd counting<br>
                    <strong>Wenzhe Zhai</strong>, Mingliang Gao, Qilei Li, Gwanggil Jeon,
                    Marco Anisetti<br>
                    <em>Applied Intelligence</em>, February 2023
                    <br>
                    [ <a href='papers/ZhaiEtAl_AI2023.pdf'>PDF</a> ]
                    [ <a
                              href='https://link.springer.com/article/10.1007/s10489-023-04499-3'>Link</a>
                    ]
                </li>
                
                <li>
                    Scale Attentive Aggregation Network for Crowd Counting and
                    Localization in Smart City<br>
                    <strong>Wenzhe Zhai</strong>, Mingliang Gao, Xiangyu Guo, Guofeng Zou, Qilei Li</strong>, Gwanggil Jeon <br>
                    <em>ACM Transactions on Sensor Networks</em>, March 2024,  <a style="color: black;"><strong>CCF-B</strong></a>
                    <br>
                    [ <a href='papers/ZhaiEtAl_ACM2024.pdf'>PDF</a> ]
                    [ <a href='https://dl.acm.org/doi/10.1145/3653454'>Link</a> ]
                </li>
                
                <li>
                    An attentive hierarchy ConvNet for crowd counting in smart city<br>
                    <strong>Wenzhe Zhai</strong>, Mingliang Gao, Alireza Souri, Qilei Li,
                    Xiangyu Guo, Jianrun Shang, Guofeng Zou<br>
                    <em>Cluster Computing</em>, April 2023
                    <br>
                    [ <a href="papers/ZhaiEtAl_CC2022.pdf">PDF</a> ]
                    [ <a
                              href="https://link.springer.com/article/10.1007/s10586-022-03749-2">Link</a>
                    ]
                </li>
                
                <li>
                    DA2Net: a dual attention-aware network for robust crowd counting<br>
                    <strong>Wenzhe Zhai</strong>, Qilei Li, Ying Zhou, Xuesong Li, Jinfeng Pan,
                    Guofeng Zou, Mingliang Gao<br>
                    <em>Multimedia Systems</em>, October 2023
                    <br>
                    [ <a href='papers/ZhaiEtAl_MS2022.pdf'>PDF</a> ]
                    [ <a
                              href='https://link.springer.com/article/10.1007/s00530-021-00877-4'>Link</a>
                    ]
                </li>

                
                <li>
                    Group-split attention network for crowd counting<br>
                    <strong>Wenzhe Zhai</strong>, Mingliang Gao, Marco Anisetti, Qilei Li,
                    Seunggil Jeon, Jinfeng Pan<br>
                    <em>Journal of Electronic Imaging</em>,
                    July 2022
                    <br>
                    [ <a href='papers/ZhaiEtAl_JET2022.pdf'>PDF</a> ]
                    [ <a
                              href='https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-31/issue-4/041214/Group-split-attention-network-for-crowd-counting/10.1117/1.JEI.31.4.041214.short'>Link</a>
                    ]
                </li>
                
                <li>
                    CT and MRI Image Fusion via Dual-branch GAN<br>
                    <strong>Wenzhe Zhai</strong>, Wenhao Song, Jinyong Chen, Guisheng Zhang, QileiLi, Mingliang Gao<br>
                    <em>International Journal of Biomedical Engineering and Technology</em>, June 2023<br>
                    [ <a href='papers/ZhaiEtAl_IJBET2023.pdf'>PDF</a> ]
                    [ <a href='https://www.inderscienceonline.com/doi/abs/10.1504/IJBET.2023.131696'>Link</a> ]
                </li>
                            
                <li>
                    Multiscale aggregation and illumination‐aware attention network for infrared and
                    visible image fusion<br>
                    Wenhao Song, <strong>Wenzhe Zhai</strong>, Mingliang Gao, Qilei Li, Abdellah
                    Chehri, Gwanggil Jeon<br>
                    <em>Concurrency and Computation: Practice and Experience</em>, April 2023
                    <br>
                    [ <a href='papers/SongEtAl_CCPE2023.pdf'>PDF</a> ]
                    [ <a href='https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7712'>Link</a> ]
                </li>
                
                <li>
                    Spatial-frequency attention network for crowd counting<br>
                    Xiangyu Guo, Mingliang Gao, <strong>Wenzhe Zhai</strong>, Jianrun Shang, Qilei Li<br>
                    <em>Big data</em>, October 2022
                    <br>
                    [ <a href="papers/GuoEtAl_BD2022.pdf">PDF</a> ]
                    [ <a href="https://www.liebertpub.com/doi/abs/10.1089/big.2022.0039">Link</a> ]
                </li>
                
                <li>
                    Scale region recognition network for object counting in intelligent
                    transportation system<br>
                    Xiangyu Guo, Mingliang Gao, <strong>Wenzhe Zhai</strong>, Qilei Li, Gwanggil
                    Jeon<br>
                    <em>IEEE Transactions on Intelligent Transportation Systems</em>, July 2023
                    <br>

                    [ <a href='papers/GuoEtAl_TITS2023.pdf'>PDF</a> ]
                    [ <a href='https://ieeexplore.ieee.org/abstract/document/10194472'>Link</a> ]
                </li>
                
                <li>
                    Dense Attention Fusion Network for Object Counting in IoT System<br>
                    Xiangyu Guo, Mingliang Gao, <strong>Wenzhe Zhai</strong>, Qilei Li, Kyu Hyung
                    Kim, Gwanggil Jeon<br>
                    <em>Mobile Networks and Applications</em>, January 2023
                    <br>
                    [ <a href='papers/GuoEtAl_MNA2023.pdf'>PDF</a> ]
                    [ <a
                              href='https://link.springer.com/article/10.1007/s11036-023-02090-1'>Link</a>
                    ]
                </li>
                
                <li>
                    Multiscale aggregation network via smooth inverse map for crowd counting<br>
                    Xiangyu Guo, Mingliang Gao, <strong>Wenzhe Zhai</strong>, Qilei Li, Jinfeng Pan,
                    Guofeng Zou<br>
                    <em>Multimedia Tools and Applications</em>, August 2022 <br>
                    [ <a href='papers/GuoEtAl_MTA2022.pdf'>PDF</a> ]
                    [ <a
                              href='https://link.springer.com/article/10.1007/s11042-022-13664-8'>Link</a>
                    ]
                </li>
                
                <li>
                    SaReGAN: a salient regional generative adversarial network for visible and
                    infrared image fusion<br>
                    Mingliang Gao, Yi’nan Zhou, <strong>Wenzhe Zhai</strong>, Shuai Zeng, Qilei Li<br>
                    <em>Multimedia Tools and Applications</em>, January 2023
                    <br>
                    [ <a href='papers/GaoEtAl_MTA2023.pdf'>PDF</a> ]
                    [ <a
                              href='https://link.springer.com/article/10.1007/s11042-023-14393-2'>Link</a>
                    ]
                </li>
                
                <li>
                    Crowd counting in smart city via lightweight Ghost Attention Pyramid Network<br>
                    Xiangyu Guo, Kai Song, Mingliang Gao, <strong>Wenzhe Zhai</strong>, Qilei Li,
                    Gwanggil Jeon<br>
                    <em>Future Generation Computer Systems</em>, October 2023
                    <br>
                    [ <a href='papers/GuoEtAl_FGCS2023.pdf'>PDF</a> ]
                    [ <a
                              href='https://www.sciencedirect.com/science/article/pii/S0167739X23001930'>Link</a>
                    ]
                </li>
                
                <li>
                    Defending Deepfakes by Saliency-Aware Attack<br>
                    Qilei Li, Mingliang Gao, Guisheng Zhang, <strong>Wenzhe Zhai</strong><br>
                    <em>IEEE Transactions on Computational Social Systems</em>, May 2023
                    <br>
                    [ <a href='papers/LiEtAl_ITCSS2023.pdf'>PDF</a> ]
                    [ <a href='https://ieeexplore.ieee.org/abstract/document/10121622'>Link</a> ]
                </li>
                
                <li>
                    A comprehensive analysis for crowd counting methodologies and algorithms in
                    Internet of Things<br>
                    Mingliang Gao, Alireza Souri, Mayram Zaker, <strong>Wenzhe Zhai</strong>, Xiangyu Guo,
                    Qilei Li<br>
                    <em>Cluster Computing</em>, March 2023
                    <br>
                    [ <a href='papers/GaoEtAl_CC2023.pdf'>PDF</a> ]
                    [ <a
                              href='https://link.springer.com/article/10.1007/s10586-023-03987-y'>Link</a>
                    ]
                </li>
                
                <li>
                    Object Counting in Remote Sensing via Selective Spatial-frequency Pyramid Network<br>
                    Jinyong Chen, Mingliang Gao, Xiangyu Guo, <strong>Wenzhe Zhai</strong>, Qilei Li, Gwanggil Jeon<br>
                    <em>Software-Practice and Experience</em>, November 2023<br>
                    [ <a href='papers/ChenEtAl_WILEY2023.pdf'>PDF</a> ]
                    [ <a href='https://onlinelibrary.wiley.com/doi/10.1002/spe.3287'>Link</a> ]
                </li>
                
                <li>
                    Towards Multimodal Disinformation Detection by Vision-language Knowledge Interaction<br>
                    Qilei Li, Mingliang Gao, Guisheng Zhang, <strong>Wenzhe Zhai</strong>, Jinyong Chen, Gwanggil Jeon<br>
                    <em>Information Fusion</em>, February 2024, <a style="color: black;"><strong>IF=18.6</strong></a><br>
                    [ <a href='papers/LiEtAl_IF2023.pdf'>PDF</a> ]
                    [ <a href='https://www.sciencedirect.com/science/article/pii/S1566253523003536'>Link</a> ]
                </li>
                
                <li>
                    Disrupting Deepfakes via Union-Saliency Adversarial Attack<br>
                    Guisheng Zhang, Mingliang Gao, Qilei Li, <strong>Wenzhe Zhai</strong>,
                    Guofeng Zou, Gwanggil Jeon<br>
                    <em>IEEE Transactions on Consumer Electronics</em>, November
                    2023
                    <br>
                    [ <a href='papers/ZhangEtAl_TCE2023.pdf'>PDF</a> ]
                    [ <a href='https://ieeexplore.ieee.org/document/10330601'>Link</a> ]
                </li>                
                
                <li>
                    Asymmetric metric learning approachbased on distribution constraints for unsupervised person re-identification<br>
                    Yue Liu, Guofeng Zou, Guizhen Chen, <strong>Wenzhe Zhai</strong>, Mingliang Gao<br>
                    <em>Control and Decision</em>, April
                    2022
                    <br>
                    [ <a href='papers/LiuEtAl_CC2022.pdf'>PDF</a> ]
                    [ <a href='http://kzyjc.alljournals.cn/kzyjc/article/abstract/2021-1598'>Link</a> ]
                </li>                
                
            </ol>
        </div>

        <!-- ============================ Workshops ======================== -->
        <div class="page-header">
            <h3>Workshop</h3>
        </div>
        <div id="latest_publications" class="container-fluid content_width_range">
            <ul>
                <li>
                    <a href="http://www.cac2021.org.cn/">China Automation Congress</a>,
                    <br>
                    A Channel-aware Attention Network for Crowd Counting
                    (<strong>CAC</strong>),
                    Beijing, China, Oct. 2021
                    <br>
                    [ <a href='papers/ZhaiEtAl_CAC2021.pdf'>PDF</a>
                    ]
                    [ <a
                              href='https://ieeexplore.ieee.org/abstract/document/9728649'>Link</a>
                    ]
                </li>

                <li>
                    <a href="https://www.iciap2021.org//">International Conference on Image Analysis and Processing</a>,
                    <br>
                    Group-split attention network for crowd counting
                    (<strong>ICIAP</strong>),
                    Lecce, Italia, May. 2022
                    <br>
                    [ <a href='papers/ZhaiEtAl_JEI2022.pdf'>PDF</a>
                    ]
                    [ <a
                              href='https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-31/issue-4/041214/Group-split-attention-network-for-crowd-counting/10.1117/1.JEI.31.4.041214.short'>Link</a>
                    ]
                </li>
                
                <li>
                    <a href="https://iothiconf.com//">International Conference on IoT and Healthcare</a>,
                    <br>
                    Group-split attention network for crowd counting
                    (<strong>IotTHIC</strong>),
                    Turkey, Oct. 2022
                    <br>
                    [ <a href='papers/ZhaiEtAl_IJBET2023.pdf'>PDF</a>
                    ]
                    [ <a
                              href='https://iothiconf.com/'>Link</a>
                    ]
                </li>
                
            </ul>
        </div>


        <!-- <div class="page-header">
            <h3>Industrial Experience</h3>
        </div>

        <div id="experience" class="container-fluid content_width_range">
            <ul>
                <li>
                    <strong>Research Assistant (Full-time)</strong><br>
                    <a href="https://nus.edu.sg/">National University of Singapore (Singapore)</a>,
                    Aug. 2020 – Oct. 2020<br>
                    Worked on real-world image super-resolution with deep learning.
                </li>
                <li>
                    <strong>Deep Learning Algorithm Engineer (Full-time)</strong><br>
                    <a href="https://uk.linkedin.com/company/vision-semantics-limited/">Vision Semantics Limited
                        (London, UK)</a>,
                    July 2022 – Oct. 2022,<br>
                    Improved person ReID framework using foundation models (ViT, CLIP, BLIP) for accurate retrieval of
                    pedestrians.
                </li>
                <li>
                    <strong>Analysis Algorithm Engineer (Part-time, 40% Full time)</strong><br>
                    <a href="https://www.veritone.com/applications/tracker/">Veritone Limited (London branch, US)</a>,
                    Oct. 2022 – present<br>
                    Implementing large-scale models for object detection, applying domain generalization
                    techniques to enhance real-world performance.
                </li>
            </ul>
        </div> -->

        <!-- <div class="page-header">
            <h3>Patents</h3>
        </div>
        <div id="Patent" class="container-fluid content_width_range">
            <ul class="patent-list">
                <li>
                    <strong>2020.08.03 - A sparse coding based infrared and visible image fusion method</strong><br>
                    <em>First applicant</em>,
                    <em>Substantive examination stage</em>,
                    <em>CN114066786A</em><br>
                    The invention discloses a fusion method of infrared image and visible image, which produces
                    excellent results with rich details and gradient information.
                </li>
                <li>
                    <strong>2019.09.16 - An image super-resolution reconstruction method based on gated multi-feedback
                        networks</strong><br>
                    <em>First applicant</em>,
                    <em>Substantive examination stage</em>,
                    <em>CN112508779A</em><br>
                    The invention presents an image super-resolution reconstruction method using gated multi-feedback
                    networks to enhance image quality.
                </li>
                <li>
                    <strong>2019.09.12 - Infrared and visible image fusion method based on coupling generation
                        adversarial network</strong><br>
                    <em>First applicant</em>,
                    <em>Substantive examination stage</em>,
                    <em>CN112488970A</em><br>
                    The invention introduces an infrared and visible image fusion method using a coupling generation
                    adversarial network to maintain thermal radiation and texture information effectively.
                </li>
                <li>
                    <strong>2019.09.16 - A multi-focus image fusion method based on sparse representation and guided
                        filtering</strong><br>
                    <em>Second applicant</em>,
                    <em>Substantive examination stage</em>,
                    <em>CN112508828A</em><br>
                    The invention discloses a multi-focus image fusion method, which effectively fuses pictures with
                    different focuses, displaying more information while maintaining sharpness and resolution.
                </li>
            </ul>
        </div> -->

        <!-- <div class="page-header"> </div>

        <div style="width: 20%; margin: 0 auto;">
            <script type="text/javascript" id="mapmyvisitors"
                src="//mapmyvisitors.com/map.js?d=OV-IJK_s6pYeenujbUNFxhjqql5f6WNp2mPSe3vxTa4&cl=ffffff&w=a"></script>
        </div> -->


        <script src="./js/jquery.js"></script>
        <script src="./js/bootstrap-transition.js"></script>
        <script src="./js/bootstrap-alert.js"></script>
        <script src="./js/bootstrap-modal.js"></script>
        <script src="./js/bootstrap-dropdown.js"></script>
        <script src="./js/bootstrap-scrollspy.js"></script>
        <script src="./js/bootstrap-tab.js"></script>
        <script src="./js/bootstrap-tooltip.js"></script>
        <script src="./js/bootstrap-popover.js"></script>
        <script src="./js/bootstrap-button.js"></script>
        <script src="./js/bootstrap-collapse.js"></script>
        <script src="./js/bootstrap-carousel.js"></script>
        <script src="./js/bootstrap-typeahead.js"></script>

</body>

<footer style="text-align: center; padding: 20px 0 0 0;">
    <a href="https://beian.miit.gov.cn/" target="_blank" style="color: #000000; text-decoration: none;">鲁ICP备2024086386号-1</a>
    <img src="images/record.png" alt="record" style="width: 18px; height: 20px;">
    <a href="https://beian.mps.gov.cn/#/query/webSearch?code=37030302001021" rel="noreferrer" target="_blank" style="color: #000000; text-decoration: none;">鲁公网安备37088102000505号</a>
</footer>

</html>